import subprocess
import sys
import torch
import platform
import re
from typing import Tuple, Optional, Dict

# å…¨å±€é…ç½®ï¼šGPUå‹å·â†’è®¡ç®—èƒ½åŠ›â†’æ¨èCUDAç‰ˆæœ¬æ˜ å°„ï¼ˆè¦†ç›–ä¸»æµNVIDIAæ˜¾å¡ï¼‰
# è®¡ç®—èƒ½åŠ›å‚è€ƒï¼šhttps://developer.nvidia.com/cuda-gpus
NVIDIA_GPU_CONFIG: Dict[str, Dict[str, str]] = {
    # RTX 40ç³»åˆ—
    "RTX 4090": {"compute_capability": "8.9", "min_cuda": "11.8", "recommend_cuda": "12.1"},
    "RTX 4080": {"compute_capability": "8.9", "min_cuda": "11.8", "recommend_cuda": "12.1"},
    "RTX 4070": {"compute_capability": "8.9", "min_cuda": "11.8", "recommend_cuda": "12.1"},
    "RTX 4060": {"compute_capability": "8.9", "min_cuda": "11.8", "recommend_cuda": "12.1"},
    "RTX 4050": {"compute_capability": "8.9", "min_cuda": "11.8", "recommend_cuda": "12.1"},
    # RTX 30ç³»åˆ—
    "RTX 3090": {"compute_capability": "8.6", "min_cuda": "11.1", "recommend_cuda": "11.8"},
    "RTX 3080": {"compute_capability": "8.6", "min_cuda": "11.1", "recommend_cuda": "11.8"},
    "RTX 3070": {"compute_capability": "8.6", "min_cuda": "11.1", "recommend_cuda": "11.8"},
    "RTX 3060": {"compute_capability": "8.6", "min_cuda": "11.1", "recommend_cuda": "11.8"},
    "RTX 3050": {"compute_capability": "8.6", "min_cuda": "11.1", "recommend_cuda": "11.8"},
    # RTX 20ç³»åˆ—
    "RTX 2080": {"compute_capability": "7.5", "min_cuda": "10.2", "recommend_cuda": "11.7"},
    "RTX 2070": {"compute_capability": "7.5", "min_cuda": "10.2", "recommend_cuda": "11.7"},
    "RTX 2060": {"compute_capability": "7.5", "min_cuda": "10.2", "recommend_cuda": "11.7"},
    # GTXç³»åˆ—
    "GTX 1660": {"compute_capability": "7.5", "min_cuda": "10.2", "recommend_cuda": "11.3"},
    "GTX 1080": {"compute_capability": "6.1", "min_cuda": "8.0", "recommend_cuda": "11.1"},
    "GTX 1070": {"compute_capability": "6.1", "min_cuda": "8.0", "recommend_cuda": "11.1"},
    "GTX 1060": {"compute_capability": "6.1", "min_cuda": "8.0", "recommend_cuda": "11.1"},
    # å…¶ä»–å¸¸è§å‹å·
    "Tesla V100": {"compute_capability": "7.0", "min_cuda": "9.0", "recommend_cuda": "11.3"},
    "Tesla T4": {"compute_capability": "7.5", "min_cuda": "10.2", "recommend_cuda": "11.7"},
}

# PyTorchä¸CUDA/ROCmç‰ˆæœ¬æ˜ å°„ï¼ˆç¡®ä¿å…¼å®¹æ€§ï¼‰
PYTORCH_VERSION_MAP: Dict[str, Dict[str, str]] = {
    "cuda": {
        "11.1": "torch==1.9.1+cu111",
        "11.3": "torch==1.12.1+cu113",
        "11.7": "torch==1.13.1+cu117",
        "11.8": "torch==2.0.1+cu118",
        "12.1": "torch==2.1.0+cu121",
    },
    "rocm": {
        "5.6": "torch==2.0.1+rocm5.6",
        "5.7": "torch==2.1.0+rocm5.7",
    },
}

# ROCmæ”¯æŒçš„AMDæ˜¾å¡ï¼ˆå‚è€ƒï¼šhttps://docs.amd.com/en/docs-versions/rocm-5.7.0/reference/gpu-accelerated.htmlï¼‰
AMD_ROCM_GPUS = [
    "Radeon RX 6000ç³»åˆ—", "Radeon RX 7000ç³»åˆ—", "Radeon Pro V620",
    "Instinct MI50", "Instinct MI60", "Instinct MI250"
]


class UniversalGPUAcceleratorChecker:
    def __init__(self):
        self.os_type = platform.system()  # Windows/Linux/Darwin(Mac)
        self.gpu_vendor = self._detect_gpu_vendor()  # NVIDIA/AMD/Unknown
        self.gpu_model = self._detect_gpu_model()  # å…·ä½“GPUå‹å·

    def _detect_gpu_vendor(self) -> str:
        """æ£€æµ‹GPUå‚å•†ï¼ˆNVIDIA/AMD/Unknownï¼‰"""
        try:
            if torch.cuda.is_available():
                return "NVIDIA"
            # æ£€æµ‹AMDæ˜¾å¡ï¼ˆWindows/Linuxï¼‰
            if self.os_type == "Windows":
                result = subprocess.check_output(
                    ["wmic", "path", "win32_videocontroller", "get", "name"],
                    text=True, stderr=subprocess.STDOUT
                )
                if any("AMD" in line or "Radeon" in line for line in result.splitlines()):
                    return "AMD"
            elif self.os_type == "Linux":
                result = subprocess.check_output(
                    ["lspci"], text=True, stderr=subprocess.STDOUT, shell=True
                )
                if any("AMD" in line or "Radeon" in line for line in result.splitlines()):
                    return "AMD"
            return "Unknown"
        except Exception:
            return "Unknown"

    def _detect_gpu_model(self) -> Optional[str]:
        """æ£€æµ‹å…·ä½“GPUå‹å·ï¼ˆå¦‚RTX 3060ã€Radeon RX 6800 XTï¼‰"""
        try:
            if self.gpu_vendor == "NVIDIA":
                if torch.cuda.is_available():
                    return torch.cuda.get_device_name(0)
                # å¤‡é€‰ï¼šé€šè¿‡nvidia-smiæ£€æµ‹
                result = subprocess.check_output(["nvidia-smi"], text=True, stderr=subprocess.STDOUT)
                for line in result.splitlines():
                    if "NVIDIA GeForce" in line or "NVIDIA RTX" in line:
                        return line.strip().split(" ")[-1]  # æå–å‹å·
            elif self.gpu_vendor == "AMD":
                if self.os_type == "Windows":
                    result = subprocess.check_output(
                        ["wmic", "path", "win32_videocontroller", "get", "name"],
                        text=True, stderr=subprocess.STDOUT
                    )
                    for line in result.splitlines():
                        if "AMD" in line or "Radeon" in line:
                            return line.strip()
                elif self.os_type == "Linux":
                    result = subprocess.check_output(
                        ["lspci | grep -i vga"], text=True, stderr=subprocess.STDOUT, shell=True
                    )
                    return result.strip()
            return None
        except Exception:
            return None

    def _get_nvidia_recommended_cuda(self) -> str:
        """æ ¹æ®NVIDIA GPUå‹å·æ¨èæœ€ä½³CUDAç‰ˆæœ¬"""
        if not self.gpu_model:
            return "11.8"  # é»˜è®¤æ¨èç¨³å®šç‰ˆ
        # æ¨¡ç³ŠåŒ¹é…GPUå‹å·ï¼ˆå¦‚"RTX 3060 Laptop"åŒ¹é…"RTX 3060"ï¼‰
        for model_pattern, config in NVIDIA_GPU_CONFIG.items():
            if model_pattern in self.gpu_model:
                return config["recommend_cuda"]
        # æœªåŒ¹é…åˆ°å…·ä½“å‹å·ï¼Œæ¨èå…¼å®¹æ€§æœ€å¹¿çš„11.8
        return "11.8"

    def _get_amd_recommended_rocm(self) -> str:
        """æ ¹æ®AMD GPUæ¨èæœ€ä½³ROCmç‰ˆæœ¬"""
        return "5.6"  # å…¼å®¹å¤§éƒ¨åˆ†AMDæ˜¾å¡çš„ç¨³å®šç‰ˆ

    def check_cuda_available(self) -> Tuple[bool, str]:
        """æ£€æŸ¥CUDA/ROCmæ˜¯å¦å¯ç”¨åŠç‰ˆæœ¬ä¿¡æ¯"""
        try:
            if self.gpu_vendor == "NVIDIA":
                # æ£€æŸ¥NVIDIA CUDA
                torch_cuda_available = torch.cuda.is_available()
                torch_cuda_version = torch.version.cuda if torch_cuda_available else None

                # æ£€æŸ¥ç³»ç»Ÿå±‚é¢CUDA
                system_cuda_available = False
                system_cuda_version = None
                try:
                    cmd = ["nvcc", "--version"]
                    if self.os_type != "Windows":
                        cmd.append("--shell")
                    result = subprocess.check_output(cmd, stderr=subprocess.STDOUT, text=True)
                    version_match = re.search(r"release (\d+\.\d+)", result)
                    if version_match:
                        system_cuda_version = version_match.group(1)
                        system_cuda_available = True
                except (subprocess.CalledProcessError, FileNotFoundError):
                    pass

                # ç»¼åˆåˆ¤æ–­
                if torch_cuda_available and system_cuda_available:
                    return True, f"PyTorch CUDA {torch_cuda_version} (ç³»ç»ŸCUDA {system_cuda_version})"
                elif system_cuda_available and not torch_cuda_available:
                    return False, f"ç³»ç»Ÿå·²å®‰è£…CUDA {system_cuda_version}ï¼Œä½†PyTorchæœªä½¿ç”¨GPUç‰ˆæœ¬"
                elif not system_cuda_available:
                    return False, "æœªæ£€æµ‹åˆ°ç³»ç»ŸCUDAç¯å¢ƒ"
                else:
                    return False, "æœªçŸ¥é”™è¯¯"

            elif self.gpu_vendor == "AMD":
                # æ£€æŸ¥AMD ROCm
                torch_rocm_available = torch.backends.rocm.is_available()
                torch_rocm_version = torch.version.rocm if torch_rocm_available else None
                if torch_rocm_available:
                    return True, f"PyTorch ROCm {torch_rocm_version}"
                else:
                    return False, "æœªæ£€æµ‹åˆ°ROCmç¯å¢ƒï¼ˆAMDæ˜¾å¡éœ€å®‰è£…ROCmæ›¿ä»£CUDAï¼‰"

            else:
                # æ— GPUæˆ–æœªçŸ¥æ˜¾å¡
                return False, "æœªæ£€æµ‹åˆ°æ”¯æŒGPUåŠ é€Ÿçš„æ˜¾å¡ï¼ˆä»…NVIDIA/AMDæ˜¾å¡æ”¯æŒï¼‰"
        except Exception as e:
            return False, f"æ£€æµ‹å¤±è´¥: {str(e)}"

    def get_gpu_info(self) -> str:
        """è·å–GPUè¯¦ç»†ä¿¡æ¯ï¼ˆå‹å·ã€æ˜¾å­˜ã€å‚å•†ï¼‰"""
        try:
            if self.gpu_vendor == "NVIDIA":
                if torch.cuda.is_available():
                    props = torch.cuda.get_device_properties(0)
                    return (
                        f"å‚å•†: NVIDIA\n"
                        f"å‹å·: {props.name}\n"
                        f"æ˜¾å­˜: {props.total_memory / 1024 ** 3:.2f}GB\n"
                        f"è®¡ç®—èƒ½åŠ›: {props.major}.{props.minor}\n"
                        f"æ¨èCUDAç‰ˆæœ¬: {self._get_nvidia_recommended_cuda()}"
                    )
                else:
                    result = subprocess.check_output(["nvidia-smi"], text=True, stderr=subprocess.STDOUT)
                    gpu_line = [line for line in result.splitlines() if "NVIDIA" in line][0]
                    return f"å‚å•†: NVIDIA\nå‹å·: {gpu_line.strip()}\næ¨èCUDAç‰ˆæœ¬: {self._get_nvidia_recommended_cuda()}"

            elif self.gpu_vendor == "AMD":
                gpu_model = self.gpu_model or "æœªçŸ¥AMDæ˜¾å¡"
                return (
                    f"å‚å•†: AMD\n"
                    f"å‹å·: {gpu_model}\n"
                    f"æ¨èROCmç‰ˆæœ¬: {self._get_amd_recommended_rocm()}\n"
                    f"æç¤º: AMDæ˜¾å¡éœ€å®‰è£…ROCmæ›¿ä»£CUDA"
                )

            else:
                return (
                    f"å‚å•†: æœªçŸ¥\n"
                    f"å‹å·: æœªæ£€æµ‹åˆ°æ”¯æŒGPUåŠ é€Ÿçš„æ˜¾å¡\n"
                    f"æç¤º: ä»…NVIDIA/AMDæ˜¾å¡æ”¯æŒGPUåŠ é€Ÿï¼Œå½“å‰ä»…èƒ½ä½¿ç”¨CPU"
                )
        except Exception as e:
            return f"è·å–GPUä¿¡æ¯å¤±è´¥: {str(e)}"

    def install_instructions(self) -> str:
        """ç”Ÿæˆé€‚é…å½“å‰ç¡¬ä»¶çš„å®‰è£…æŒ‡å¯¼"""
        instructions = []
        if self.gpu_vendor == "NVIDIA":
            cuda_version = self._get_nvidia_recommended_cuda()
            instructions.append(f"=== æ¨èå®‰è£…CUDA {cuda_version}ï¼ˆé€‚é…ä½ çš„{self.gpu_model}ï¼‰===")

            if self.os_type == "Windows":
                instructions.append(f"1. ä¸‹è½½CUDA {cuda_version}å®‰è£…åŒ…:")
                instructions.append(f"   https://developer.nvidia.com/cuda-{cuda_version}-0-download-archive")
                instructions.append("2. å®‰è£…æ—¶å‹¾é€‰:")
                instructions.append("   - CUDA Runtime (å¿…éœ€)")
                instructions.append("   - cuDNN (å¯é€‰ä½†æ¨èï¼ŒåŠ é€Ÿæ¨¡å‹æ¨ç†)")
                instructions.append("   - ç¬”è®°æœ¬ç”¨æˆ·åŠ¡å¿…å‹¾é€‰'ç¬”è®°æœ¬ä¼˜åŒ–'")
                instructions.append("3. éªŒè¯å®‰è£…:")
                instructions.append("   æ‰“å¼€å‘½ä»¤æç¤ºç¬¦ï¼Œè¾“å…¥: nvcc --version")

            elif self.os_type == "Linux":
                instructions.append(f"1. è¿è¡Œè‡ªåŠ¨å®‰è£…è„šæœ¬ï¼ˆUbuntu/Debianï¼‰:")
                instructions.append(
                    f"   wget https://developer.download.nvidia.com/compute/cuda/{cuda_version}.0/local_installers/cuda_{cuda_version}.0_525.85.12_linux.run")
                instructions.append(f"   sudo sh cuda_{cuda_version}.0_525.85.12_linux.run")
                instructions.append("2. é…ç½®ç¯å¢ƒå˜é‡ï¼ˆæ·»åŠ åˆ°~/.bashrcæˆ–~/.zshrcï¼‰:")
                instructions.append(f"   echo 'export PATH=/usr/local/cuda-{cuda_version}/bin:$PATH' >> ~/.bashrc")
                instructions.append(
                    f"   echo 'export LD_LIBRARY_PATH=/usr/local/cuda-{cuda_version}/lib64:$LD_LIBRARY_PATH' >> ~/.bashrc")
                instructions.append("   source ~/.bashrc")

            # PyTorchå®‰è£…æŒ‡ä»¤
            torch_cmd = PYTORCH_VERSION_MAP["cuda"].get(cuda_version, PYTORCH_VERSION_MAP["cuda"]["11.8"])
            instructions.append(f"\n=== å®‰è£…GPUç‰ˆPyTorchï¼ˆé€‚é…CUDA {cuda_version}ï¼‰===")
            instructions.append(
                f"pip install {torch_cmd} torchvision torchaudio --index-url https://download.pytorch.org/whl/cu{cuda_version.replace('.', '')}")

        elif self.gpu_vendor == "AMD":
            rocm_version = self._get_amd_recommended_rocm()
            instructions.append(f"=== æ¨èå®‰è£…ROCm {rocm_version}ï¼ˆé€‚é…ä½ çš„AMDæ˜¾å¡ï¼‰===")

            if self.os_type == "Linux":
                # AMD ROCmä»…æ”¯æŒLinux
                instructions.append("1. å®‰è£…ROCmä¾èµ–ï¼ˆUbuntu 20.04/22.04ï¼‰:")
                instructions.append("   sudo apt update && sudo apt install wget gnupg2")
                instructions.append(f"   wget https://repo.radeon.com/rocm/rocm.gpg.key -O - | sudo apt-key add -")
                instructions.append(
                    f"   echo 'deb [arch=amd64] https://repo.radeon.com/rocm/apt/{rocm_version} focal main' | sudo tee /etc/apt/sources.list.d/rocm.list")
                instructions.append("   sudo apt update && sudo apt install rocm-hip-sdk rocm-opencl-sdk")
                instructions.append("2. é…ç½®ç¯å¢ƒå˜é‡:")
                instructions.append("   echo 'export PATH=/opt/rocm/bin:$PATH' >> ~/.bashrc")
                instructions.append("   echo 'export LD_LIBRARY_PATH=/opt/rocm/lib:$LD_LIBRARY_PATH' >> ~/.bashrc")
                instructions.append("   source ~/.bashrc")

                # PyTorchå®‰è£…æŒ‡ä»¤
                torch_cmd = PYTORCH_VERSION_MAP["rocm"][rocm_version]
                instructions.append(f"\n=== å®‰è£…GPUç‰ˆPyTorchï¼ˆé€‚é…ROCm {rocm_version}ï¼‰===")
                instructions.append(
                    f"pip install {torch_cmd} torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm{rocm_version.replace('.', '')}")
            else:
                instructions.append("âš ï¸  æ³¨æ„ï¼šAMD ROCmä»…æ”¯æŒLinuxç³»ç»Ÿï¼ŒWindows/MacOSæš‚ä¸æ”¯æŒGPUåŠ é€Ÿ")

        else:
            instructions.append("=== æ— å¯ç”¨GPUåŠ é€Ÿæ–¹æ¡ˆ ===")
            instructions.append("å½“å‰è®¾å¤‡æœªæ£€æµ‹åˆ°æ”¯æŒGPUåŠ é€Ÿçš„æ˜¾å¡ï¼ˆä»…NVIDIA/AMDæ˜¾å¡æ”¯æŒï¼‰")
            instructions.append("å»ºè®®ï¼šä½¿ç”¨CPUæ¨¡å¼è¿è¡Œï¼Œæˆ–æ›´æ¢NVIDIA/AMDæ˜¾å¡")

        return "\n".join(instructions)

    def verify_installation(self) -> bool:
        """éªŒè¯GPUåŠ é€Ÿæ˜¯å¦é…ç½®æˆåŠŸ"""
        if self.gpu_vendor == "NVIDIA":
            return torch.cuda.is_available()
        elif self.gpu_vendor == "AMD":
            return torch.backends.rocm.is_available()
        else:
            return False

    def run_full_check(self) -> None:
        """è¿è¡Œå®Œæ•´æ£€æµ‹+å®‰è£…æŒ‡å¯¼æµç¨‹"""
        print("=" * 50)
        print("ğŸ¯ é€šç”¨GPUåŠ é€Ÿç¯å¢ƒæ£€æµ‹å·¥å…·ï¼ˆæ”¯æŒNVIDIA/AMDå…¨ç³»åˆ—æ˜¾å¡ï¼‰")
        print("=" * 50)

        # 1. æ˜¾ç¤ºGPUåŸºç¡€ä¿¡æ¯
        print("\nğŸ“Š GPUç¡¬ä»¶ä¿¡æ¯:")
        print("-" * 30)
        print(self.get_gpu_info())

        # 2. æ£€æŸ¥åŠ é€Ÿç¯å¢ƒçŠ¶æ€
        print("\nğŸ” åŠ é€Ÿç¯å¢ƒæ£€æµ‹ç»“æœ:")
        print("-" * 30)
        cuda_available, cuda_msg = self.check_cuda_available()
        print(f"åŠ é€ŸçŠ¶æ€: {'âœ… å¯ç”¨' if cuda_available else 'âŒ ä¸å¯ç”¨'}")
        print(f"è¯¦ç»†ä¿¡æ¯: {cuda_msg}")

        # 3. ç”Ÿæˆå®‰è£…æŒ‡å¯¼ï¼ˆä»…å½“åŠ é€Ÿä¸å¯ç”¨æ—¶ï¼‰
        if not cuda_available and self.gpu_vendor in ["NVIDIA", "AMD"]:
            print("\nğŸ“‹ é€‚é…å®‰è£…æŒ‡å¯¼:")
            print("-" * 30)
            print(self.install_instructions())

            # 4. è¯¢é—®æ˜¯å¦éªŒè¯å®‰è£…ï¼ˆLinuxç³»ç»Ÿæ”¯æŒç›´æ¥æ‰§è¡Œå‘½ä»¤ï¼‰
            if self.os_type == "Linux":
                confirm = input("\næ˜¯å¦è¦åœ¨ç»ˆç«¯ä¸­æ˜¾ç¤ºå®Œæ•´å®‰è£…å‘½ä»¤ï¼Ÿ(y/n): ").strip().lower()
                if confirm == "y":
                    print("\nğŸ’» å®Œæ•´å®‰è£…å‘½ä»¤:")
                    print("-" * 30)
                    if self.gpu_vendor == "NVIDIA":
                        cuda_version = self._get_nvidia_recommended_cuda()
                        print(f"# å®‰è£…CUDA {cuda_version}")
                        print(
                            f"wget https://developer.download.nvidia.com/compute/cuda/{cuda_version}.0/local_installers/cuda_{cuda_version}.0_525.85.12_linux.run")
                        print(f"sudo sh cuda_{cuda_version}.0_525.85.12_linux.run")
                        print(f"echo 'export PATH=/usr/local/cuda-{cuda_version}/bin:$PATH' >> ~/.bashrc")
                        print(
                            f"echo 'export LD_LIBRARY_PATH=/usr/local/cuda-{cuda_version}/lib64:$LD_LIBRARY_PATH' >> ~/.bashrc")
                        print("source ~/.bashrc")
                        # PyTorchå‘½ä»¤
                        torch_cmd = PYTORCH_VERSION_MAP["cuda"][cuda_version]
                        print(f"\n# å®‰è£…PyTorchï¼ˆé€‚é…CUDA {cuda_version}ï¼‰")
                        print(
                            f"pip install {torch_cmd} torchvision torchaudio --index-url https://download.pytorch.org/whl/cu{cuda_version.replace('.', '')}")
                    elif self.gpu_vendor == "AMD":
                        rocm_version = self._get_amd_recommended_rocm()
                        print("# å®‰è£…ROCm 5.6")
                        print("sudo apt update && sudo apt install wget gnupg2")
                        print(f"wget https://repo.radeon.com/rocm/rocm.gpg.key -O - | sudo apt-key add -")
                        print(
                            f"echo 'deb [arch=amd64] https://repo.radeon.com/rocm/apt/{rocm_version} focal main' | sudo tee /etc/apt/sources.list.d/rocm.list")
                        print("sudo apt update && sudo apt install rocm-hip-sdk rocm-opencl-sdk")
                        print("echo 'export PATH=/opt/rocm/bin:$PATH' >> ~/.bashrc")
                        print("echo 'export LD_LIBRARY_PATH=/opt/rocm/lib:$LD_LIBRARY_PATH' >> ~/.bashrc")
                        print("source ~/.bashrc")
                        # PyTorchå‘½ä»¤
                        torch_cmd = PYTORCH_VERSION_MAP["rocm"][rocm_version]
                        print(f"\n# å®‰è£…PyTorchï¼ˆé€‚é…ROCm {rocm_version}ï¼‰")
                        print(
                            f"pip install {torch_cmd} torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm{rocm_version.replace('.', '')}")

        # 5. æœ€ç»ˆéªŒè¯ç»“æœ
        print("\n" + "=" * 50)
        print("âœ… æ£€æµ‹å®Œæˆï¼")
        if self.verify_installation():
            print("ğŸ‰ GPUåŠ é€Ÿç¯å¢ƒå·²å°±ç»ªï¼Œå¯æ­£å¸¸ç”¨äºidenseæ£€ç´¢ç­‰æ¨¡å—ï¼")
        else:
            print("âš ï¸  è¯·æŒ‰ç…§ä¸Šè¿°æŒ‡å¯¼å®Œæˆå®‰è£…ï¼Œå®‰è£…åé‡æ–°è¿è¡Œæœ¬å·¥å…·éªŒè¯é…ç½®ã€‚")
        print("=" * 50)


if __name__ == "__main__":
    checker = UniversalGPUAcceleratorChecker()
    checker.run_full_check()